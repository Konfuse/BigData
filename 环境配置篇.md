# 环境配置篇

完整的`/etc/profile`文件：

```shell
# set java environment
JAVA_HOME=/usr/java/jdk1.8.0_201
JRE_HOME=/usr/java/jdk1.8.0_201/jre
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME JRE_HOME CLASSPATH PATH

# set flink environment
export FLINK_HOME=/usr/local/flink/flink-1.7.2
export PATH=$PATH:$FLINK_HOME/bin

# set maven environment
export M2_HOME=/opt/maven
export CLASSPATH=$CLASSPATH:$M2_HOME/lib
export PATH=$PATH:$M2_HOME/bin

# set hadoop environment
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# set hbase environment
export HBASE_HOME=/usr/local/hbase-1.3.3
export PATH=$PATH:$HBASE_HOME/bin

# set zookeeper environment
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin

# set kafka environment
export KAFKA_HOME=/usr/local/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```



## 1. Java

首先解压Java文件到`/usr/java`目录下

```sh
sudo tar -zxvf jdk-8u201-linux-x64.tar.gz -C /usr/java
```

加入环境变量，在`/etc/profile`文件中加入Java的PATH：

```shell
# set java environment
export JAVA_HOME=/usr/java/jdk1.8.0_201
export JRE_HOME=/usr/java/jdk1.8.0_201/jre
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
```

刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

## 2. Hadoop

首先解压Hadoop文件到`/usr/local/`目录下

```shell
sudo tar -zxvf hadoop-2.6.5.tar.gz -C /usr/local
```

重命名并修改权限：

```shell
cd /usr/local
sudo mv ./hadoop-2.6.5/ ./hadoop
sudo chown -R konfuse /hadoop
```

加入环境变量，在`/etc/profile`文件中加入hadoop的PATH：

```shell
# set hadoop environment
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

### 一. 伪分布式配置：

伪分布式需要配置`/usr/local/hadoop/etc/hadoop`下的三个配置文件：

#### 1. `hadoop-env.sh`配置

在其中加入计算机当前的Java环境：

```shell
# The java implementation to use.
export JAVA_HOME=/usr/java/jdk1.8.0_201
```

#### 2. `core-site.xml`配置

vim打开文件，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/usr/local/hadoop/tmp</value>
        </property>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://localhost:9000</value>
        </property>
</configuration>
```

说明：可以看到这里使用的是`hdfs://localhost:9000`的hdfs文件系统，其它还有淘宝的`tfs://`，谷歌的`gfs://`以及本地的`file://`。

#### 3. `hdfs-site.xml`配置

vim打开文件，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
               <name>dfs.namenode.name.dir</name>
               <value>/usr/local/hadoop/tmp/dfs/name</value>
        </property>
        <property>
               <name>dfs.datanode.data.dir</name>
               <value>/usr/local/hadoop/tmp/dfs/data</value>
        </property>
</configuration>
```

其中，`dfs.replication`指的是文件的备份数。

**配置说明：** 

- Hadoop的运行方式由配置文件决定，Hadoop运行时会读取配置文件，因此删除`core-site.xml`中的配置项，就可以从伪分布模式切换到非分布式模式。
- 官方教程中，伪分布式只需要配置`fs.defaultFS`和`dfs.replication`就可以运行，但是默认使用的临时目录为`/tmp/hadoop-hadoop`，而这个目录在重启时有可能被系统清理掉，导致必须重新执行format才行。
- 所以实际使用需要配置`hadoop.tmp.dir`，`dfs.namenode.name.dir`和`dfs.datanode.data.dir`。

配置成功，启动hadoop之前需要对namenode进行初始化：

```shell
hdfs namenode -format
```

然后开启NameNode和DatNode进程：

```shell
start-dfs.sh
```

**注意：** 每次使用`hdfs namenode -format`命令进行初始化时，都会为NameNode生成新的namespaceID，但是在目录`hadoop.tmp.dir`中还是保留上次的namespaceID，若namespaceID不一致，则DataNode无法启动。所以需要删除目录`/usr/local/hadoop/tmp/dfs`下的data，重新生成datanode。

关闭Hadoop：

```shell
stop-dfs.sh
```

### 二. 集群配置：

第一步对IP，Host，ssh进行配置，使集群之间可以互相通讯。可以在master进行修改，完成之后备份到其它机器上。

第二步修改Hadoop的配置文件，满足集群环境要求。

#### 1. 集群网络配置

- 修改主机名，把三台虚拟机分别命名为：<br>主节点：master <br>从节点1：slaver1 <br>从节点2：slaver2：

  ```shell
  sudo vim /etc/hostname
  ```

- 接下来打开hosts文件，将三台虚拟机的ip地址和主机名追加在里面：

  ```shell
  sudo vim /etc/hosts
  
  192.168.92.138	master
  192.168.92.139	slaver1
  192.168.92.140	slaver2
  ```

  三台虚拟机的hosts文件一样，配置完成之后可以互相通讯。

- 配置三台虚拟机的ssh，并设置免密登录，首先完成安装openssh-server包：

  ```shell
  sudo apt install openssh-server
  ```

#### 2. Hadoop配置

集群的配置主要涉及对`/usr/local/hadoop/etc/hadoop`下六个文件的修改：

`hadoop-env.sh`配置

在其中加入计算机当前的Java环境：

```shell
# The java implementation to use.
export JAVA_HOME=/usr/java/jdk1.8.0_201
# The hadoop configuration to use.
export HADOOP_CONF_DIR=/home/hadoop/hadoop-2.7.7/etc/hadoop
```

`core-site.xml`配置

vim打开文件，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
	<!--1.A base for other temporary directories.-->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop/tmp</value>
	</property>
    <!--2.the default file system-->
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
</configuration>
```

`hdfs-site.xml`配置

vim打开文件，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
	<!--1.where the DFS name node should store the name table(fsimage)-->
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>/usr/local/hadoop/tmp/dfs/name</value>
	</property>
	<!--2.where an DFS data node should store its blocks.-->
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>/usr/local/hadoop/tmp/dfs/data</value>
	</property>
	<!--3.Default block replication.-->
	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>
	<!--4.enable permission checking in HDFS-->
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
</configuration>
```

`mapred-site.xml`配置

文件本身不存在，从`mapred-site.xml.template`文件复制新文件为`mapred-site.xml`，然后vim打开，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
	<!--1.The runtime framework for executing MapReduce jobs.-->
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	<!--2.MapReduce JobHistory Server IPC host:port-->
	<property>
		<name>mapreduce.jobhistory.address</name>
		<value>master:10020</value>
	</property>
	<!--3.MapReduce JobHistory Server Web UI host:port-->
	<property>
		<name>mapreduce.jobhistory.webapp.address</name>
		<value>master:19888</value>
	</property>
</configuration>
```

`yarn-site.xml`配置

vim打开文件，在`<configuration></configuration>`中添加如下配置：

```xml
<configuration>
	<!--0.A comma separated list of services...-->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<!--1.The address of the applications manager interface in the RM.-->
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>master:8032</value>
	</property>
	<!--2.The address of the scheduler interface.-->
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>master:8030</value>
	</property>
	<!--3.resource-tracker-->
	<property>	
		<name>yarn.resourcemanager.resource-tracker.address</name>	
		<value>master:8031</value>	
	</property>	
	<!--4.The address of the RM admin interface.-->
	<property>
		<name>yarn.resourcemanager.admin.address</name>
		<value>master:8033</value>
	</property>
	<!--5.The http address of the RM web application.-->
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>master:8088</value>
	</property>	
	<!--6.Whether to enable log aggregation-->
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>	
</configuration>
```

`slaves`配置

打开`slaves`文件，在其中加入两台从主机的名字：

```
slaver1
slaver2
```

## 3. Zookeeper

第一步，解压压缩包到`/usr/local`目录下，更名为zookeeper，并分配用户权限

```shell
sudo tar -zxvf zookeeper-3.4.13.tar.gz -C /usr/local
sudo mv zookeeper-3.4.13/ zookeeper
sudo chown -R konfuse zookeeper/
```

第二步，将zookeeper加入环境变量，在`/etc/profile`文件中加入zookeeper的PATH：

```shell
# set zookeeper environment
export ZOOKEEPER_HOME=/usr/local/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

​	刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

第三步，创建zookeeper的核心文件`zoo.cfg`，初次使用 zookeeper 时，需要将 `$ZOOKEEPER_HOME/conf `目录下的 `zoo_sample.cfg `重命名为 `zoo.cfg`

```shell
cd /usr/local/zookeeper/conf
cp zoo_sample.cfg zoo.cfg
```

` zoo.cfg `作为zookeeper的配置文件，默认配置如下:

```shell
# The number of milliseconds of each 
ticktickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.dataDir=/tmp/zookeeper
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
```

**配置说明**：

- tickTime: ZooKeeper 中使用的基本时间单元，以毫秒为单位，默认值是 2000。它用来调节心跳和超时。例如，默认的会话超时时间是两倍的 tickTime。

- initLimit: 默认值是 10，即 tickTime 属性值的 10 倍。它用于配置允许 followers 连接并同步到 leader 的最大时间。如果 ZooKeeper 管理的数据量很大的话可以增加这个值。

- syncLimit: 默认值是 5，即 tickTime 属性值的 5 倍。它用于配置leader 和 followers 间进行心跳检测的最大延迟时间。如果在设置的时间内 followers 无法与 leader 进行通信，那么 followers 将会被丢弃。

- dataDir: ZooKeeper 用来存储内存数据库快照的目录，并且除非指定其它目录，否则数据库更新的事务日志也将会存储在该目录下。建议配置 dataLogDir 参数来指定 ZooKeeper 事务日志的存储目录。

- clientPort: 服务器监听客户端连接的端口，也即客户端尝试连接的端口，默认值是 2181。

- maxClientCnxns: 在 socket 级别限制单个客户端与单台服务器之前的并发连接数量，可以通过 IP 地址来区分不同的客户端。它用来防止某种类型的 DoS 攻击，包括文件描述符耗尽。默认值是 60。将其设置为 0 将完全移除并发连接数的限制。

- autopurge.snapRetainCount: 配置 ZooKeeper 在自动清理的时候需要保留的数据文件快照的数量和对应的事务日志文件，默认值是 3。

- autopurge.purgeInterval: 和参数 autopurge.snapRetainCount 配套使用，用于配置 ZooKeeper 自动清理文件的频率，默认值是 1，即默认开启自动清理功能，设置为 0 则表示禁用自动清理功能。

### 一. 单机模式

#### 1. zoo.cfg配置

```shell
ticketTime=2000
clientPort=2181
dataDir=/usr/local/zookeeper/data
dataLogDir=/usr/local/zookeeper/logs
```

#### 2. 启动zookeeper服务

```shell
zkServer.sh start
```

#### 3. 使用telnet 和 stat 命令验证服务是否成功启动

```shell
telnet 127.0.0.1 2181
```

​	在单机模式中，Mode 的值是 "standalone"。

#### 4. 停止zookeeper服务

```shell
zkServer.sh stop
```

### 二. 集群模式

#### 1. zoo.cfg配置

在单机模式的配置文件下增加了最后 5 行配置:

```shell
ticketTime=2000
clientPort=2181
dataDir=/usr/local/zookeeper/data
dataLogDir=/usr/local/zookeeper/logs
initLimit=10
syncLimit=5
server.1=master:2888:3888
server.2=slaver1:2888:3888
server.3=slaver2:2888:3888
```

**配置说明：**

- 集群模式中，集群中的每台机器都需要感知其它机器， 在 `zoo.cfg `配置文件中，可以按照如下格式进行配置，每一行代表一台服务器配置:

  ```shell
  server.id=host:port:port
  ```

- id 被称为 Server ID，用来标识服务器在集群中的序号。同时每台 ZooKeeper 服务器上，都需要在数据目录(即 dataDir 指定的目录) 下创建一个 `myid `文件，该文件只有一行内容，即对应于每台服务器的Server ID。
- ZooKeeper 集群中，每台服务器上的` zoo.cfg` 配置文件内容一致。
- server.1 的` myid `文件内容就是 "1"。每个服务器的 myid 内容都不同，且需要保证和自己的 `zoo.cfg` 配置文件中 `server.id=host:port:port `的 id 值一致。
- id 的范围是 1 ~ 255。

#### 2. 创建myid文件

在 dataDir 指定的目录下 (即 `/usr/local/zookeeper/data `目录) 创建名为` myid `的文件，文件内容和` zoo.cfg` 中当前机器的 id 一致。根据上述配置, master 的 `myid `文件内容为 1。

```shell
cd /usr/local/zookeeper
vim myid
```

#### 3. slave配置

按照相同步骤，为 slaver1 和 slaver2 配置 `zoo.cfg `和` myid `文件。`zoo.cfg`文件内容相同，slaver1 的 `myid `文件内容为 2，slaver2 的` myid `文件内容为 3。

#### 4. 启动zookeeper服务

在集群中的每台机器上执行以下启动命令:

```shell
zkServer.sh start
```

启动后使用jps查看进程，如果看到QuorumPeerMain进程,说明这个机器上的zookeeper就启动了。

同理，依次启动所有机器上的zookeeper。接着每个机器可以使用命令查看自己的状态

## 4. HBase

第一步，解压压缩包到`/usr/local`目录下

```shell
sudo tar -zxvf hbase-1.3.3-bin.tar.gz -C /usr/local
```

第二步，将hbase加入环境变量，在`/etc/profile`文件中加入hbase的PATH：

```shell
# set hbase environment
export HBASE_HOME=/usr/local/hbase-1.3.3
export PATH=$PATH:$HBASE_HOME/bin
```

​	刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

第三步，修改hbase zk模式，在`/usr/local/hbase-1.3.3/conf/hbase-env.sh`文件中加入（如果已有配置好的zookeeper集群，那么跳过这一步。）

```shell
export  HBASE_MANAGES_ZK=true
```

第四步，修改`/usr/local/hbase-1.3.3/conf/hbase-site.xml`，在`<configuration></configuration>`中做如下修改：

```xml
<configuration>
　　<property> 
　　　　<name>hbase.rootdir</name> 
　　　　<value>hdfs://master:9000/hbase</value> 
　　</property> 
　　<property> 
　　　　<name>hbase.cluster.distributed</name> 
　　　　<value>true</value> 
　　</property> 
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
　　<property> 
　　　　<name>hbase.zookeeper.quorum</name> 
　　　　<value>master,slaver1,slaver2</value> 
　　</property> 
　　<property> 
　　　　<name>hbase.zookeeper.property.dataDir</name> 
　　　　<value>/usr/local/zookeeper/data</value> 
　　</property>
     <property>
           <name>hbase.master.maxclockskew</name>
           <value>120000</value>
     </property>
</configuration>
```

第五步，修改`regionservers`配置文件

添加：

```
master
slaver1
slaver2
```

第六步，将配置同步给其它节点。

第七步，启动HBase之前必须要启动HDFS，然后在启动HBase

```shell
start-hbase.sh
```

第八步，关闭HBase。

```shell
stop-hbase.sh
```

## 5. Maven

第一步，解压压缩包到`/opt`目录下，并更名为maven

```shell
sudo tar -zxvf apache-maven-3.6.0-bin.tar.gz -C /opt
sudo mv apache-maven-3.6.0/ maven
```

第二步，将maven加入环境变量，在`/etc/profile`文件中加入maven的PATH：

```shell
# set maven environment
export M2_HOME=/opt/maven
export CLASSPATH=$CLASSPATH:$M2_HOME/lib
export PATH=$PATH:$M2_HOME/bin
```

​	刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

第三步，验证是否配置成功

```shell
mvn -v
```

​	显示maven版本即安装成功。

第四步，配置maven本地仓库位置及更换默认镜像为163源

- 在`/home`目录下创建`/maven/repository`作为maven本地仓库存储路径，并分配权限

```shell
sudo mkdir -p /home/maven/repository
cd /home
sudo chown -R konfuse maven/
```

- 在`opt/maven/conf/settings/xml`中加入配置，修改本地仓库位置与镜像源

```xml
<!-- localRepository
   | The path to the local repository maven will use to store artifacts.
   |
   | Default: ${user.home}/.m2/repository
  <localRepository>/path/to/local/repo</localRepository>
  -->

<localRepository>/home/maven/repository</localRepository>
...
...
<mirrors>
    <!-- mirror
     | Specifies a repository mirror site to use instead of a given repository. The repository that
     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used
     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.
     |
    <mirror>
      <id>mirrorId</id>
      <mirrorOf>repositoryId</mirrorOf>
      <name>Human Readable Name for this Mirror.</name>
      <url>http://my.repository.com/repo/path</url>
    </mirror>
     -->
    <mirror>
        <id>nexus-aliyun</id>
        <name>nexus-aliyun</name>
        <url>
        http://maven.aliyun.com/nexus/content/groups/public
        </url>
        <mirrorOf>central</mirrorOf>
    </mirror>
</mirrors>

```

## 6. Flink

>  Flink 有两种部署的模式，分别是 Standalone 以及 Yarn Cluster 模式。
>
> -  Standalone模式：Flink 必须依赖于 Zookeeper 来实现 JobManager 的 HA（Zookeeper 已经成为了大部分开源框架 HA 必不可少的模块）。在 Zookeeper 的帮助下，一个 Standalone 的 Flink 集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后（如宕机或 Crash），Zookeeper 会从 Standby 中选举新的 JobManager 来接管 Flink 集群。
>
> - Yarn Cluaster 模式：Flink 就要依靠 Yarn 本身来对 JobManager 做 HA 了。其实这里完全是 Yarn 的机制。对于 Yarn Cluster 模式来说，JobManager 和 TaskManager 都是被 Yarn 启动在 Yarn 的 Container 中。此时的 JobManager，其实应该称之为 Flink Application Master。也就说它的故障恢复，就完全依靠着 Yarn 中的 ResourceManager（和 MapReduce 的 AppMaster 一样）。由于完全依赖了 Yarn，因此不同版本的 Yarn 可能会有细微的差异。这里不再做深究。

这里使用standalone模式

第一步，创建目录`/usr/local/flink`，解压压缩包到`/usr/local/flink`目录下，并分配用户权限：

```shell
sudo tar -zxvf flink-1.7.2-bin-hadoop26-scala_2.12.tgz -C /usr/local/flink
cd /usr/local
sudo chown -R konfuse flink/
```

第二步，加入环境变量，在`/etc/profile`文件中加入flink的PATH：

```shell
# set flink environment
export FLINK_HOME=/usr/local/flink/flink-1.7.2
export PATH=$PATH:$FLINK_HOME/bin
```

​	刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

第三步，在目录`/usr/local/flink/conf/`修改配置文件`masters`，`slaves`，`flink-conf.yaml`，并同步给所有的节点：

### 1. `masters`配置

vim打开文件，加入：

```shell
master:8081
```

### 2. `slaves`配置

vim打开文件，加入：

```shell
slaver1
slaver2
```

### 3. `flink-conf.yaml`配置

vim打开文件，修改：

```shell
# JobManager runs.
jobmanager.rpc.address: master

# The RPC port where the JobManager is reachable.
jobmanager.rpc.port: 6123


# The heap size for the JobManager JVM
jobmanager.heap.size: 1024m


# The heap size for the TaskManager JVM
taskmanager.heap.size: 2048m


# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.
taskmanager.numberOfTaskSlots: 8

# The parallelism used for programs that did not specify and other parallelism.
parallelism.default: 1

...
...

# The port under which the web-based runtime monitor listens.
# A value of -1 deactivates the web server
rest.port: 8081
```

### 4. 启动flink

```shell
start-cluster.sh
```

## 7. Kafka

第一步，解压压缩包到`/usr/local`目录下，重命名，并分配用户权限

```shell
sudo tar -zxvf kafka_2.11-0.11.0.3.tgz -C /usr/local
cd /usr/local
sudo mv kafka_2.11-0.11.0.3/ kafka
sudo chown -R konfuse kafka/
```

第二步，将kafka加入环境变量，在`/etc/profile`文件中加入kafka的PATH：

```shell
# set kafka environment
export KAFKA_HOME=/usr/local/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

​	刷新当前环境，或重启计算机：

```shell
source /etc/profile
```

第三步，修改配置文件`server.properties`：

```
broker.id=0 #每个kafka节点的唯一标识
listeners=PLAINTEXT://192.168.5.28:9092 #监听端口 
log.dirs=/data/kafka-logs #日志地址
zookeeper.connect=master:2181,slaver1:2181,slaver2:2181
```

第四步，启动kafka，和zookeeper一样，每个节点都要启动

```shell
kafka-server-start.sh -daemon server.properties
```

第五步，关闭Kafka集群，和zookeeper一样, 在所有机器上运行stop脚本

```shell
kafka-server-stop.sh
```